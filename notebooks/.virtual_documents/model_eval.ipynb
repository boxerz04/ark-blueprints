


# === CONFIG ===
APPROACH = "base"  # "base" or "sectional"

# Derived paths
DATA_DIR   = f"data/processed/{APPROACH}"
MODEL_DIR  = f"models/{APPROACH}/latest"

# Optional caps for permutation importance and SHAP sampling
PI_N_REPEATS   = 5
PI_MAX_SAMPLES = 20000  # larger -> slower
SHAP_MAX_SAMPLES = 2000  # if shap is available

print("APPROACH =", APPROACH)
print("DATA_DIR =", DATA_DIR)
print("MODEL_DIR =", MODEL_DIR)



# === Robust loader (repo root autodetect + clear checks) ===
import os, json
import numpy as np
import pandas as pd
from pathlib import Path
from scipy import sparse
from scipy.sparse import load_npz
import joblib

from sklearn.metrics import roc_auc_score, average_precision_score, log_loss, accuracy_score, matthews_corrcoef
from sklearn.inspection import permutation_importance

print("CWD =", os.getcwd())

def find_repo_root(start: Path) -> Path:
    p = start.resolve()
    for q in [p] + list(p.parents):
        if (q/"data").exists() and (q/"models").exists():
            return q
    return p  # fallback

REPO = find_repo_root(Path.cwd())
print("REPO =", REPO)

DATA_DIR = (REPO / DATA_DIR).resolve()
MODEL_DIR = (REPO / MODEL_DIR).resolve()
print("DATA_DIR =", DATA_DIR)
print("MODEL_DIR =", MODEL_DIR)

# --- X / y ---
x_candidates = [DATA_DIR/"X.npz", DATA_DIR/"X_dense.npz"]
x_path = next((p for p in x_candidates if p.exists()), None)
if x_path is None:
    raise FileNotFoundError(
        f"特徴量が見つかりませんでした。\n"
        f"探したパス: {[str(p) for p in x_candidates]}\n"
        f"まず前処理を実行してください：\n"
        f"  python scripts\\preprocess_base_features.py --master data\\processed\\master.csv "
        f"--out-dir data\\processed\\{APPROACH} --pipeline-dir models\\{APPROACH}\\latest"
    )

if x_path.name == "X_dense.npz":
    arr = np.load(x_path)["X"]
    X = sparse.csr_matrix(arr)
else:
    X = load_npz(x_path)

y_path = DATA_DIR/"y.csv"
if not y_path.exists():
    raise FileNotFoundError(f"y.csv が見つかりません: {y_path}\n"
                            "preprocess_base_features.py の実行を確認してください。")
y = pd.read_csv(y_path)["is_top2"].to_numpy().astype(int)

# --- pipeline / model ---
pipe_path = MODEL_DIR / "feature_pipeline.pkl"
if not pipe_path.exists():
    raise FileNotFoundError(f"feature_pipeline.pkl が見つかりません: {pipe_path}\n"
                            "学習前処理の出力先（models/{APPROACH}/latest）を確認してください。")
pipe_obj = joblib.load(pipe_path)
ct = pipe_obj["column_transformer"] if isinstance(pipe_obj, dict) and "column_transformer" in pipe_obj else pipe_obj

def find_model(md: Path):
    for name in ["model.pkl","model.joblib","model.bin"]:
        p = md / name
        if p.exists():
            return p
    return None

model_path = find_model(MODEL_DIR)
if model_path is None:
    raise FileNotFoundError(f"学習済みモデルが見つかりません: {MODEL_DIR}\n"
                            "train.py を実行して latest を更新してください。")

model = joblib.load(model_path)

X.shape, y.shape, type(ct), type(model), str(x_path), str(model_path)



from sklearn.compose import ColumnTransformer

def get_feature_names_from_ct(ct) -> list[str]:
    names = []
    try:
        for name, trans, cols in ct.transformers_:
            if name == "remainder" and trans == "drop":
                continue
            if hasattr(trans, "named_steps"):
                last = None
                for _, step in trans.named_steps.items():
                    last = step
                if hasattr(last, "get_feature_names_out"):
                    feats = last.get_feature_names_out(cols)
                    names.extend([str(x) for x in feats])
                    continue
            if hasattr(trans, "get_feature_names_out"):
                feats = trans.get_feature_names_out(cols)
                names.extend([str(x) for x in feats])
            else:
                names.extend([str(c) for c in cols])
        try:
            names2 = ct.get_feature_names_out()
            if len(names2) == len(names):
                names = [str(x) for x in names2]
        except Exception:
            pass
    except Exception as e:
        print("WARN: feature name recovery failed:", e)
        names = [f"feat_{i}" for i in range(X.shape[1])]
    return names

feature_names = get_feature_names_from_ct(ct)
len(feature_names), feature_names[:10]



import numpy as np

def safe_predict_proba(model, X):
    if hasattr(model, "predict_proba"):
        return model.predict_proba(X)[:, 1]
    elif hasattr(model, "decision_function"):
        z = model.decision_function(X)
        return 1.0 / (1.0 + np.exp(-z))
    else:
        yp = model.predict(X)
        return np.clip(yp.astype(float), 0.0, 1.0)

y_prob = safe_predict_proba(model, X)
y_pred = (y_prob >= 0.5).astype(int)

metrics = {
    "auc": float(roc_auc_score(y, y_prob)),
    "pr_auc": float(average_precision_score(y, y_prob)),
    "logloss": float(log_loss(y, y_prob, labels=[0,1])),
    "accuracy": float(accuracy_score(y, y_pred)),
    "mcc": float(matthews_corrcoef(y, y_pred)),
    "n_rows": int(X.shape[0]),
    "n_features": int(X.shape[1]),
}
metrics









import pandas as pd

if hasattr(model, "feature_importances_"):
    fi = pd.DataFrame({"feature": feature_names, "importance": model.feature_importances_})
    fi = fi.sort_values("importance", ascending=False).reset_index(drop=True)
    fi.head(20)
else:
    print("model has no feature_importances_")



display(fi.head(30))


import matplotlib.pyplot as plt

if hasattr(model, "feature_importances_"):
    topk = 30
    top = fi.head(topk).iloc[::-1]
    plt.figure(figsize=(8, max(4, topk*0.3)))
    plt.barh(top["feature"], top["importance"])
    plt.title(f"Top {topk} Feature Importances")
    plt.tight_layout()
    plt.show()









from sklearn.inspection import permutation_importance
from scipy import sparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

rng = np.random.RandomState(42)
n = X.shape[0]
max_n = max(1000, min(PI_MAX_SAMPLES, n))
idx = rng.choice(n, size=max_n, replace=False)

# --- permutation_importance は sparse を受け付けない → dense に変換 ---
Xs = X[idx] if sparse.issparse(X) else X[idx, :]
ys = y[idx]

# メモリ/速度対策：sparse→dense 変換を安全に
def to_dense_safe(M, max_try=3):
    if not sparse.issparse(M):
        return np.asarray(M)
    for _ in range(max_try):
        try:
            return M.toarray()
        except MemoryError:
            # サンプルを半分にして再トライ
            m = M.shape[0] // 2
            M = M[:m]
    # 最後の手段：さらにサンプルを絞って返す
    return M[: min(2000, M.shape[0])].toarray()

Xs_dense = to_dense_safe(Xs)

pi = permutation_importance(
    model, Xs_dense, ys, scoring="neg_log_loss",
    n_repeats=PI_N_REPEATS, random_state=42, n_jobs=-1
)

pi_df = pd.DataFrame({
    "feature": feature_names,
    "pi_mean": pi.importances_mean,
    "pi_std": pi.importances_std
}).sort_values("pi_mean", ascending=False).reset_index(drop=True)

display(pi_df.head(30))

topk = 30
top = pi_df.head(topk).iloc[::-1]
plt.figure(figsize=(8, max(4, topk*0.3)))
plt.barh(top["feature"], top["pi_mean"])
plt.title(f"Top {topk} Permutation Importance (neg_log_loss)")
plt.tight_layout()
plt.show()









# === Fast SHAP for tree-based models ===
import numpy as np
import shap
from scipy import sparse

# 1) サンプル数を絞る（例: 1000）
k = min(1000, X.shape[0])
rng = np.random.RandomState(0)
idx = rng.choice(X.shape[0], size=k, replace=False)

Xk = X[idx]
if sparse.issparse(Xk):
    # メモリ対策：どうしても厳しければ 1000 → 600 等に落とす
    Xk = Xk.toarray()

# 2) 背景を圧縮（shap.sample or kmeans）
bg = shap.sample(Xk, 100)  # or: shap.kmeans(Xk, 50)

# 3) TreeExplainer（確率出力・加法性チェックオフで高速化）
explainer = shap.TreeExplainer(
    model,
    data=bg,
    feature_perturbation="interventional",
    model_output="probability",
)
sv = explainer(Xk, check_additivity=False)

# 4) 可視化
#shap.plots.bar(sv, max_display=30, show=True)
#shap.plots.beeswarm(sv, max_display=30, show=True)



import numpy as np
from scipy import sparse
import shap

# 1) Xk を SHAP実行時と同じものに揃える（疎なら密へ）
Xdata = Xk.toarray() if sparse.issparse(Xk) else Xk

# 2) 列数の整合チェック
assert sv.values.shape[1] == len(feature_names), \
    f"SHAP列={sv.values.shape[1]} と feature_names={len(feature_names)} が不一致"

# 3) 名前とデータを付けて Explanation を作り直す
sv = shap.Explanation(
    values=sv.values,
    base_values=sv.base_values,
    data=Xdata,
    feature_names=feature_names,
)

# 4) これで“名前スライス”が使えます
shap.plots.bar(sv, max_display=30, show=True)  # お試し
shap.plots.beeswarm(sv, max_display=30, show=True)
# 単一特徴の依存プロット（展示残差）
shap.plots.scatter(sv[:, "num__tenji_resid"], color=sv)

# 相互作用っぽさ（色に ST_tenji を乗せる）
shap.plots.scatter(sv[:, "num__tenji_resid"], color=sv[:, "num__ST_tenji"])




