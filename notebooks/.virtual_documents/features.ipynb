


from pathlib import Path

def find_project_root(start: Path) -> Path:
    """
    現在ディレクトリから親に向かって 'data' と 'models' を両方含む場所をルートとみなす。
    見つからなければカレントを返す（保険）。
    """
    for p in [start] + list(start.parents):
        if (p / "data").exists() and (p / "models").exists():
            return p
    return start

# ノートブック実行時は CWD（作業ディレクトリ）起点でOK
PROJECT_ROOT = find_project_root(Path.cwd())

DATA_DIR  = PROJECT_ROOT / "data" / "processed"
MODEL_DIR = PROJECT_ROOT / "models" / "base" / "latest"   # ← ここを models/base/latest に変更！

DATA_DIR.mkdir(parents=True, exist_ok=True)
MODEL_DIR.mkdir(parents=True, exist_ok=True)

MASTER_CSV = DATA_DIR / "master.csv"

print("PROJECT_ROOT:", PROJECT_ROOT)
print("MASTER_CSV exists?", MASTER_CSV.exists())






import pandas as pd

df = pd.read_csv(MASTER_CSV, encoding="utf-8-sig", parse_dates=["date"])
print(df.shape)
df.head(3)






# ターゲット/ID/リーク列の整理 ===
TARGET = "is_top2"

ID_COLS   = ["race_id", "player", "player_id", "motor_number", "boat_number", "section_id"]  # 学習に入れない
LEAK_COLS = ["entry", "is_wakunari", "rank", "winning_trick", "remarks", "henkan_ticket", "ST", "ST_rank", "__source_file"]

y   = df[TARGET].astype(int)
ids = df[ID_COLS].astype(str)

used = df.drop(columns=ID_COLS + LEAK_COLS + [TARGET], errors="ignore").copy()

# 最小の整合性チェック（変換はしない）
assert "wakuban" in used.columns, "wakuban が見当たりません（preprocessの出力を確認）"
assert used["wakuban"].between(1, 6).all(), "wakuban に 1–6 以外の値があります"
assert "ST_tenji" in used.columns, "ST_tenji が見当たりません"

print("used shape:", used.shape)
print("y balance:", y.value_counts().to_dict())






# === セル3：時系列ソート（レース単位）→ y / ids / used を再構成 ===
import numpy as np
import pandas as pd

# 代表日付：各レースの最小date（同一race_idでも念のため）
race_date = df.groupby("race_id")["date"].min()

# タイブレーク用に code, R（存在すれば）も持ってくる
meta = df.groupby("race_id")[["code","R"]].min() if {"code","R"}.issubset(df.columns) else pd.DataFrame(index=race_date.index)

race_order = (
    race_date.to_frame("race_date")
             .join(meta)
             .sort_values(["race_date","code","R"], na_position="last")
             .index.to_numpy()
)

# 上記の順に race を並べ、行（艇）は wakuban 昇順で固定
df_sorted = (
    df.set_index("race_id")
      .loc[race_order]
      .reset_index()
      .sort_values(["date","race_id","wakuban"])
      .reset_index(drop=True)
)

# 並び替え後に改めて y / ids / used を作り直す（重要！）
y   = df_sorted[TARGET].astype(int).copy()
ids = df_sorted[ID_COLS].astype(str).copy()
used = df_sorted.drop(columns=ID_COLS + LEAK_COLS + [TARGET], errors="ignore").copy()

# 簡易チェック
print("[check] date monotonic? ", bool(df_sorted["date"].is_monotonic_increasing))
if "wakuban" in df_sorted.columns:
    gb = df_sorted.groupby("race_id")["wakuban"].count()
    print("[check] rows per race (head):")
    print(gb.head())
    assert (gb >= 2).all(), "レース内の行数が極端に少ないものがあります（データを確認）"

print("[ok] sorted shapes  used:", used.shape, " y:", y.shape, " ids:", ids.shape)






# === 数値=全部 / カテゴリ=SAFE＋低カーディナリティ自動追加 → 明示ドロップ ===
from pandas.api.types import is_numeric_dtype

# 1) 数値は全部
NUM_COLS = [c for c in used.columns if is_numeric_dtype(used[c])]

# 2) カテゴリ：SAFE + 低カーディナリティ（<=50）を自動追加
SAFE_CAT = [
    # team, origin, timetable, title, schedule, precondition_1/2, propeller, parts_exchange は入れない
    "AB_class","place","weather","wind_direction","sex",
    "race_grade","race_type","race_attribute",
]
MAX_CAT_CARD = 50

obj_cols        = used.select_dtypes(include="object").columns.tolist()
safe_present    = [c for c in SAFE_CAT if c in obj_cols]
auto_candidates = [c for c in obj_cols if c not in safe_present]
auto_card       = used[auto_candidates].nunique(dropna=True).sort_values(ascending=False)
auto_add        = auto_card[auto_card <= MAX_CAT_CARD].index.tolist()

CAT_COLS = sorted(set(safe_present + auto_add))

# 3) 今は使わない列を明示ドロップ
DROP_FEATS = [
    "origin","team","parts_exchange","title","schedule","timetable",
    "precondition_1","precondition_2","propeller",
]
NUM_COLS = [c for c in NUM_COLS if c not in DROP_FEATS]
CAT_COLS = [c for c in CAT_COLS if c not in DROP_FEATS]

print(f"NUM_COLS ({len(NUM_COLS)}):", NUM_COLS[:10], "...")
print(f"CAT_COLS ({len(CAT_COLS)}):", CAT_COLS[:10], "...")
print(f"[drop now] {sorted([c for c in DROP_FEATS if c in used.columns])}")

# 4) 採用した CAT_COLS のユニーク数を出力
if CAT_COLS:
    cat_card = used[CAT_COLS].nunique(dropna=True).sort_values(ascending=False)
    print("\n[CAT_COLS cardinality]\n", cat_card.to_string())
else:
    print("\n[CAT_COLS cardinality] (none)")



len(NUM_COLS)


NUM_COLS





# === セル4：前処理パイプライン構築（scikit-learn バージョン差異対応） ===
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# 1) 数値列の処理
num_tf = Pipeline(steps=[
    ("scaler", StandardScaler())
])

# 2) カテゴリ列の処理（One-Hot）
# scikit-learn 1.2 以降は sparse_output 引数、それ以前は sparse
try:
    cat_tf = Pipeline(steps=[
        ("ohe", OneHotEncoder(
            handle_unknown="ignore",   # 未知カテゴリを無視（安全）
            sparse_output=True         # v1.2+
        ))
    ])
except TypeError:
    cat_tf = Pipeline(steps=[
        ("ohe", OneHotEncoder(
            handle_unknown="ignore",   # 未知カテゴリを無視（安全）
            sparse=True                # v1.1 以下
        ))
    ])

# 3) 全体をまとめる
preprocessor = ColumnTransformer(
    transformers=[
        ("num", num_tf, NUM_COLS),
        ("cat", cat_tf, CAT_COLS),
    ],
    remainder="drop"
)

print(f"[Pipeline] num={len(NUM_COLS)} cat={len(CAT_COLS)}")






# === fit_transform（形状・疎密・y分布の確認） ===
import numpy as np

X = preprocessor.fit_transform(used)  # 前処理を学習してから変換
print(type(X), X.shape)

# y（目的変数）の分布
print("y balance:", y.value_counts().to_dict())






# === セル6：特徴・ターゲット・ID・前処理パイプを保存 ===
from scipy import sparse
from scipy.sparse import save_npz
import joblib

# 出力ディレクトリ（data/processed / models/latest）
OUT_DIR = DATA_DIR              # → data/processed
PIPELINE_DIR = MODEL_DIR        # → models/latest
OUT_DIR.mkdir(parents=True, exist_ok=True)
PIPELINE_DIR.mkdir(parents=True, exist_ok=True)

# 特徴量行列 X の保存（疎行列 or 密行列で分岐）
if sparse.issparse(X):
    save_npz(OUT_DIR / "X.npz", X)
    x_path = OUT_DIR / "X.npz"
else:
    np.savez_compressed(OUT_DIR / "X_dense.npz", X=X)
    x_path = OUT_DIR / "X_dense.npz"

# y, ids の保存
y_path   = OUT_DIR / "y.csv"
ids_path = OUT_DIR / "ids.csv"
y.to_csv(y_path, index=False, encoding="utf-8-sig")
ids.to_csv(ids_path, index=False, encoding="utf-8-sig")

# 前処理器（最新を models/latest/ に保存）
pipeline_path = PIPELINE_DIR / "feature_pipeline.pkl"
joblib.dump(preprocessor, pipeline_path)

# ログ出力
print("[OK] 保存が完了しました")
print(f" - X:        {x_path} (shape={X.shape}, sparse={sparse.issparse(X)})")
print(f" - y:        {y_path}")
print(f" - ids:      {ids_path}")
print(f" - pipeline: {pipeline_path}")






# === 時系列ホールドアウト（前80%→学習 / 後20%→検証）AUC + Top-2 ===
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score
import joblib
import numpy as np

# 1) レース順（セル3の df_sorted が時系列になっている前提）
rid_order = df_sorted["race_id"].astype(str).to_numpy()
uniq_rids = []
seen = set()
for rid in rid_order:
    if rid not in seen:
        seen.add(rid)
        uniq_rids.append(rid)
uniq_rids = np.array(uniq_rids)

# 2) 80/20 でレースを分割（行ではなくレース単位）
split_at = int(len(uniq_rids) * 0.8)
train_rids = set(uniq_rids[:split_at])
valid_rids = set(uniq_rids[split_at:])

m_tr = np.array([rid in train_rids for rid in rid_order])
m_va = ~m_tr

tr_idx = np.where(m_tr)[0]
va_idx = np.where(m_va)[0]

print(f"[split] races train/valid = {len(train_rids)} / {len(valid_rids)}")
print(f"[split] rows   train/valid = {len(tr_idx)} / {len(va_idx)}")

# 3) 保存済みパイプラインをロード
preprocessor = joblib.load(MODEL_DIR / "feature_pipeline.pkl")

# 4) 変換
Xtr = preprocessor.transform(used.iloc[tr_idx])
Xva = preprocessor.transform(used.iloc[va_idx])

y_np = y.to_numpy(dtype=int)
ytr, yva = y_np[tr_idx], y_np[va_idx]
rid_va = rid_order[va_idx]

# 5) 学習と評価
clf = LGBMClassifier(
    n_estimators=400, learning_rate=0.05, num_leaves=63,
    subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1
)
clf.fit(Xtr, ytr)

proba = clf.predict_proba(Xva)[:, 1]
auc   = roc_auc_score(yva, proba)

# Top-2 ヒット率
hits = []
for rid in np.unique(rid_va):
    m = (rid_va == rid)
    if m.sum() < 2:
        continue
    order = np.argsort(proba[m])[::-1]
    top2  = yva[m][order[:2]]
    hits.append(int(top2.sum() > 0))
top2 = float(np.mean(hits)) if hits else float("nan")

print(f"[TimeHoldout] AUC={auc:.4f}  Top-2Hit={top2:.4f}")






# === セル8：特徴量重要度（LightGBM） ===
import matplotlib.pyplot as plt

# 特徴量名（引数なしで呼ぶ）
feature_names = preprocessor.get_feature_names_out()

# 重要度を取得
importances = clf.feature_importances_
indices = np.argsort(importances)[::-1]

# 上位20を可視化
plt.figure(figsize=(10,6))
plt.barh(range(20), importances[indices[:20]][::-1])
plt.yticks(range(20), feature_names[indices[:20]][::-1])
plt.xlabel("Importance")
plt.title("LightGBM Feature Importances (Top 20)")
plt.show()



feature_names


import numpy as np

feature_names = preprocessor.get_feature_names_out()
importances = clf.feature_importances_
idx = np.where(feature_names == "num__wakuban")[0]
if len(idx):
    i = idx[0]
    rank = int(np.argsort(importances)[::-1].tolist().index(i) + 1)
    print(f"wakuban importance: {importances[i]:.1f}  (rank {rank}/{len(feature_names)})")
else:
    print("wakuban feature not found in transformed names")



used.info()



