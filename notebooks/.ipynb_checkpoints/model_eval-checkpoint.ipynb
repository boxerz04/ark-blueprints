{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation Notebook\n",
        "\n",
        "- Date: 2025-10-09\n",
        "- Purpose: Evaluate trained models (base / sectional), inspect feature names, feature importances, permutation importance, and SHAP (if installed).\n",
        "\n",
        "> Modify the CONFIG cell to switch approach/paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# === CONFIG ===\n",
        "APPROACH = \"base\"  # \"base\" or \"sectional\"\n",
        "\n",
        "# Derived paths\n",
        "DATA_DIR   = f\"data/processed/{APPROACH}\"\n",
        "MODEL_DIR  = f\"models/{APPROACH}/latest\"\n",
        "\n",
        "# Optional caps for permutation importance and SHAP sampling\n",
        "PI_N_REPEATS   = 5\n",
        "PI_MAX_SAMPLES = 20000  # larger -> slower\n",
        "SHAP_MAX_SAMPLES = 2000  # if shap is available\n",
        "\n",
        "print(\"APPROACH =\", APPROACH)\n",
        "print(\"DATA_DIR =\", DATA_DIR)\n",
        "print(\"MODEL_DIR =\", MODEL_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy import sparse\n",
        "from scipy.sparse import load_npz\n",
        "import joblib\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss, accuracy_score, matthews_corrcoef\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "DATA_DIR = Path(DATA_DIR)\n",
        "MODEL_DIR = Path(MODEL_DIR)\n",
        "\n",
        "# Load X / y\n",
        "if (DATA_DIR / \"X.npz\").exists():\n",
        "    X = load_npz(DATA_DIR / \"X.npz\")\n",
        "elif (DATA_DIR / \"X_dense.npz\").exists():\n",
        "    arr = np.load(DATA_DIR / \"X_dense.npz\")[\"X\"]\n",
        "    X = sparse.csr_matrix(arr)\n",
        "else:\n",
        "    raise FileNotFoundError(\"X(.npz) not found in \" + str(DATA_DIR))\n",
        "\n",
        "y = pd.read_csv(DATA_DIR / \"y.csv\")[\"is_top2\"].to_numpy().astype(int)\n",
        "\n",
        "# Load pipeline and model\n",
        "pipe_path = MODEL_DIR / \"feature_pipeline.pkl\"\n",
        "pipe_obj = joblib.load(pipe_path)\n",
        "ct = pipe_obj[\"column_transformer\"] if isinstance(pipe_obj, dict) and \"column_transformer\" in pipe_obj else pipe_obj\n",
        "\n",
        "def find_model(md: Path):\n",
        "    for name in [\"model.pkl\",\"model.joblib\",\"model.bin\"]:\n",
        "        p = md / name\n",
        "        if p.exists():\n",
        "            return p\n",
        "    raise FileNotFoundError(\"model file not found in \" + str(md))\n",
        "\n",
        "model = joblib.load(find_model(MODEL_DIR))\n",
        "\n",
        "X.shape, y.shape, type(ct), type(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "def get_feature_names_from_ct(ct) -> list[str]:\n",
        "    names = []\n",
        "    try:\n",
        "        for name, trans, cols in ct.transformers_:\n",
        "            if name == \"remainder\" and trans == \"drop\":\n",
        "                continue\n",
        "            if hasattr(trans, \"named_steps\"):\n",
        "                last = None\n",
        "                for _, step in trans.named_steps.items():\n",
        "                    last = step\n",
        "                if hasattr(last, \"get_feature_names_out\"):\n",
        "                    feats = last.get_feature_names_out(cols)\n",
        "                    names.extend([str(x) for x in feats])\n",
        "                    continue\n",
        "            if hasattr(trans, \"get_feature_names_out\"):\n",
        "                feats = trans.get_feature_names_out(cols)\n",
        "                names.extend([str(x) for x in feats])\n",
        "            else:\n",
        "                names.extend([str(c) for c in cols])\n",
        "        try:\n",
        "            names2 = ct.get_feature_names_out()\n",
        "            if len(names2) == len(names):\n",
        "                names = [str(x) for x in names2]\n",
        "        except Exception:\n",
        "            pass\n",
        "    except Exception as e:\n",
        "        print(\"WARN: feature name recovery failed:\", e)\n",
        "        names = [f\"feat_{i}\" for i in range(X.shape[1])]\n",
        "    return names\n",
        "\n",
        "feature_names = get_feature_names_from_ct(ct)\n",
        "len(feature_names), feature_names[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def safe_predict_proba(model, X):\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        return model.predict_proba(X)[:, 1]\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        z = model.decision_function(X)\n",
        "        return 1.0 / (1.0 + np.exp(-z))\n",
        "    else:\n",
        "        yp = model.predict(X)\n",
        "        return np.clip(yp.astype(float), 0.0, 1.0)\n",
        "\n",
        "y_prob = safe_predict_proba(model, X)\n",
        "y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "metrics = {\n",
        "    \"auc\": float(roc_auc_score(y, y_prob)),\n",
        "    \"pr_auc\": float(average_precision_score(y, y_prob)),\n",
        "    \"logloss\": float(log_loss(y, y_prob, labels=[0,1])),\n",
        "    \"accuracy\": float(accuracy_score(y, y_pred)),\n",
        "    \"mcc\": float(matthews_corrcoef(y, y_pred)),\n",
        "    \"n_rows\": int(X.shape[0]),\n",
        "    \"n_features\": int(X.shape[1]),\n",
        "}\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Native Feature Importances (if available)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "if hasattr(model, \"feature_importances_\"):\n",
        "    fi = pd.DataFrame({\"feature\": feature_names, \"importance\": model.feature_importances_})\n",
        "    fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
        "    fi.head(20)\n",
        "else:\n",
        "    print(\"model has no feature_importances_\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if hasattr(model, \"feature_importances_\"):\n",
        "    topk = 30\n",
        "    top = fi.head(topk).iloc[::-1]\n",
        "    plt.figure(figsize=(8, max(4, topk*0.3)))\n",
        "    plt.barh(top[\"feature\"], top[\"importance\"])\n",
        "    plt.title(f\"Top {topk} Feature Importances\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Permutation Importance (model-agnostic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.RandomState(42)\n",
        "n = X.shape[0]\n",
        "max_n = max(1000, min(PI_MAX_SAMPLES, n))\n",
        "idx = rng.choice(n, size=max_n, replace=False)\n",
        "Xs = X[idx] if sparse.issparse(X) else X[idx, :]\n",
        "ys = y[idx]\n",
        "\n",
        "pi = permutation_importance(model, Xs, ys, scoring=\"neg_log_loss\",\n",
        "                            n_repeats=PI_N_REPEATS, random_state=42, n_jobs=-1)\n",
        "pi_df = pd.DataFrame({\"feature\": feature_names, \"pi_mean\": pi.importances_mean, \"pi_std\": pi.importances_std})\n",
        "pi_df = pi_df.sort_values(\"pi_mean\", ascending=False).reset_index(drop=True)\n",
        "display(pi_df.head(20))\n",
        "\n",
        "topk = 30\n",
        "top = pi_df.head(topk).iloc[::-1]\n",
        "plt.figure(figsize=(8, max(4, topk*0.3)))\n",
        "plt.barh(top[\"feature\"], top[\"pi_mean\"])\n",
        "plt.title(f\"Top {topk} Permutation Importance (neg_log_loss)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SHAP (if installed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "try:\n",
        "    import shap\n",
        "    import numpy as np\n",
        "\n",
        "    k = min(SHAP_MAX_SAMPLES, X.shape[0])\n",
        "    rng = np.random.RandomState(0)\n",
        "    idx = rng.choice(X.shape[0], size=k, replace=False)\n",
        "    Xk = X[idx]\n",
        "\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        model_fn = lambda data: model.predict_proba(data)[:, 1]\n",
        "    else:\n",
        "        model_fn = lambda data: model.decision_function(data)\n",
        "\n",
        "    try:\n",
        "        explainer = shap.Explainer(model, feature_names=feature_names)\n",
        "        sh = explainer(Xk)\n",
        "        shap_values = sh.values\n",
        "        shap.plots.bar(shap_values, max_display=30, show=True)\n",
        "        shap.plots.beeswarm(shap_values, max_display=30, show=True)\n",
        "    except Exception as e:\n",
        "        print(\"Auto Explainer failed:\", e)\n",
        "        print(\"Falling back to KernelExplainer (slow) ...\")\n",
        "        explainer = shap.KernelExplainer(model_fn, Xk[:200])\n",
        "        shap_values = explainer.shap_values(Xk, nsamples=100)\n",
        "        shap.summary_plot(shap_values, feature_names=feature_names, max_display=30, show=True)\n",
        "except Exception as e:\n",
        "    print(\"SHAP not available or failed:\", e)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}